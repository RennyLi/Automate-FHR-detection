{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set outcome\n",
    "outcome = 'pH'\n",
    "\n",
    "# Threshold to define outcome as abnormal or not\n",
    "outcome_threshold = 7.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    '''Singleton object for storing paths to data and database.'''\n",
    "\n",
    "    data = './ctu-data_csv'\n",
    "    meta = 'metadata.csv'\n",
    "\n",
    "\n",
    "paths = Paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1012', '1006', '1210', '1204', '2043', '1238', '1402', '1364', '1370', '1416', '1358', '1199', '1166', '1172', '1173', '1167', '1198', '1359', '1371', '1417', '1403', '1365', '1239', '2042', '1205', '1211', '1007', '1013', '1005', '1011', '1039', '1207', '1213', '2040', '1398', '1415', '1373', '1367', '1401', '1429', '1171', '1165', '1159', '1158', '1164', '1170', '1428', '1366', '1400', '1414', '1372', '1399', '2041', '1212', '1206', '1038', '1010', '1004', '1028', '1014', '2045', '1202', '1216', '1389', '1438', '1376', '1410', '1404', '1362', '1148', '1174', '1160', '1161', '1175', '1149', '1405', '1363', '1377', '1411', '1439', '1388', '1217', '1203', '2044', '1015', '1001', '1029', '1017', '1003', '1229', '2046', '1215', '1201', '1349', '1361', '1407', '1413', '1375', '1188', '1163', '1177', '1176', '1162', '1189', '1412', '1374', '1360', '1406', '1348', '1200', '1214', '1228', '1002', '1016', '1071', '1065', '1059', '1298', '1273', '1501', '2008', '1267', '2020', '2034', '1307', '1461', '1475', '1313', '1449', '1105', '1111', '1139', '1138', '1110', '1104', '1448', '1474', '1312', '1306', '1460', '2035', '2021', '2009', '1500', '1266', '1272', '1299', '1058', '1064', '1070', '1099', '1066', '1072', '1264', '1502', '1270', '1258', '2037', '2023', '1489', '1310', '1476', '1462', '1304', '1338', '1112', '1106', '1107', '1113', '1339', '1463', '1305', '1311', '1477', '1488', '2022', '1259', '2036', '1271', '1265', '1503', '1073', '1067', '1098', '1088', '1063', '1077', '2032', '1249', '2026', '1261', '1275', '1498', '1329', '1473', '1315', '1301', '1467', '1117', '1103', '1102', '1116', '1300', '1466', '1472', '1314', '1328', '1499', '1274', '1506', '1260', '1248', '2027', '2033', '1076', '1062', '1089', '1048', '1074', '1060', '1289', '2025', '2031', '2019', '1276', '1262', '1504', '1458', '1464', '1302', '1316', '1470', '1128', '1100', '1114', '1115', '1101', '1129', '1317', '1471', '1465', '1303', '1459', '1263', '1505', '2018', '1277', '2030', '2024', '1288', '1061', '1075', '1049', '1093', '1087', '1050', '1044', '1078', '1291', '1285', '1252', '2029', '1246', '2001', '2015', '1483', '1497', '1326', '1440', '1454', '1332', '1468', '1124', '1130', '1118', '1119', '1131', '1125', '1469', '1455', '1333', '1327', '1441', '1496', '1482', '2014', '2028', '1247', '1253', '1284', '1290', '1079', '1045', '1051', '1086', '1092', '1084', '1090', '1047', '1053', '1286', '1292', '1245', '1251', '1279', '2016', '2002', '1494', '1480', '1331', '1457', '1443', '1325', '1319', '1133', '1127', '1126', '1132', '1318', '1442', '1324', '1330', '1456', '1481', '1495', '2003', '1278', '2017', '1250', '1244', '1293', '1287', '1052', '1046', '1091', '1085', '1081', '1095', '1042', '1056', '1283', '1297', '2013', '1268', '2007', '1240', '1254', '1491', '1485', '1308', '1452', '1334', '1320', '1446', '1136', '1122', '1123', '1137', '1321', '1447', '1453', '1335', '1309', '1484', '1490', '1255', '1241', '1269', '2006', '2012', '1296', '1282', '1057', '1043', '1094', '1080', '1096', '1082', '1069', '1055', '1041', '1294', '1280', '2004', '2010', '2038', '1257', '1243', '1486', '1492', '1479', '1445', '1323', '1337', '1451', '1109', '1121', '1135', '1134', '1120', '1108', '1336', '1450', '1444', '1322', '1478', '1493', '1487', '1242', '2039', '1256', '2011', '2005', '1281', '1295', '1040', '1054', '1068', '1083', '1097', '1033', '1027', '1231', '1225', '1219', '1386', '1392', '1423', '1345', '1351', '1437', '1379', '1184', '1190', '1147', '1153', '1152', '1146', '1191', '1185', '1378', '1350', '1436', '1422', '1344', '1393', '1387', '1218', '1224', '1230', '1026', '1032', '1024', '1030', '1018', '1226', '1232', '1391', '1385', '1434', '1352', '1346', '1420', '1408', '1193', '1187', '1150', '1144', '1178', '1179', '1145', '1151', '1186', '1192', '1409', '1347', '1421', '1435', '1353', '1384', '1390', '1233', '1227', '1019', '1031', '1025', '1009', '1021', '1035', '1223', '1237', '1394', '1380', '1419', '1357', '1431', '1425', '1343', '1196', '1182', '1169', '1155', '1141', '1140', '1154', '1168', '1183', '1197', '1424', '1342', '1356', '1430', '1418', '1381', '1395', '1236', '1222', '1034', '1020', '1008', '1036', '1022', '1208', '1234', '1220', '1383', '1397', '1368', '1340', '1426', '1432', '1354', '1181', '1195', '1142', '1156', '1157', '1143', '1194', '1180', '1433', '1355', '1341', '1427', '1369', '1396', '1382', '1221', '1235', '1209', '1023', '1037'])\n"
     ]
    }
   ],
   "source": [
    "# Set up dictionary for CSV files\n",
    "csv_files = dict()\n",
    "\n",
    "# Load files into dictionary, but remove metadata\n",
    "filenames = os.listdir(paths.data)\n",
    "filenames.remove(paths.meta)\n",
    "for file in filenames:\n",
    "    # Get filename and file extension\n",
    "    filename, file_extension = os.path.splitext(os.path.join(paths.data, file))\n",
    "    # Load data and save to dict with filename (without path/csv) as index\n",
    "    if file_extension == '.csv':\n",
    "        filename_short = filename.split(os.sep)[-1]\n",
    "        csv_files[filename_short] =  pd.read_csv(os.path.join(paths.data, file))\n",
    "        \n",
    "print(csv_files.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1220</th>\n",
       "      <th>1234</th>\n",
       "      <th>1208</th>\n",
       "      <th>1038</th>\n",
       "      <th>1004</th>\n",
       "      <th>1010</th>\n",
       "      <th>1022</th>\n",
       "      <th>1036</th>\n",
       "      <th>2041</th>\n",
       "      <th>1206</th>\n",
       "      <th>...</th>\n",
       "      <th>1079</th>\n",
       "      <th>1290</th>\n",
       "      <th>1284</th>\n",
       "      <th>1077</th>\n",
       "      <th>1063</th>\n",
       "      <th>1088</th>\n",
       "      <th>1253</th>\n",
       "      <th>1247</th>\n",
       "      <th>2028</th>\n",
       "      <th>2014</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parameter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>7.30</td>\n",
       "      <td>7.29</td>\n",
       "      <td>7.23</td>\n",
       "      <td>7.33</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.35</td>\n",
       "      <td>7.28</td>\n",
       "      <td>7.08</td>\n",
       "      <td>7.37</td>\n",
       "      <td>7.24</td>\n",
       "      <td>...</td>\n",
       "      <td>7.17</td>\n",
       "      <td>7.36</td>\n",
       "      <td>7.16</td>\n",
       "      <td>7.22</td>\n",
       "      <td>7.16</td>\n",
       "      <td>7.25</td>\n",
       "      <td>7.32</td>\n",
       "      <td>7.32</td>\n",
       "      <td>7.18</td>\n",
       "      <td>7.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BDecf</th>\n",
       "      <td>3.52</td>\n",
       "      <td>2.50</td>\n",
       "      <td>5.84</td>\n",
       "      <td>2.72</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.20</td>\n",
       "      <td>1.53</td>\n",
       "      <td>8.11</td>\n",
       "      <td>3.69</td>\n",
       "      <td>2.06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.91</td>\n",
       "      <td>3.88</td>\n",
       "      <td>5.07</td>\n",
       "      <td>6.69</td>\n",
       "      <td>5.56</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>4.82</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pCO2</th>\n",
       "      <td>6.00</td>\n",
       "      <td>6.50</td>\n",
       "      <td>6.60</td>\n",
       "      <td>5.70</td>\n",
       "      <td>5.50</td>\n",
       "      <td>4.70</td>\n",
       "      <td>7.00</td>\n",
       "      <td>9.30</td>\n",
       "      <td>4.80</td>\n",
       "      <td>7.70</td>\n",
       "      <td>...</td>\n",
       "      <td>7.10</td>\n",
       "      <td>4.90</td>\n",
       "      <td>8.50</td>\n",
       "      <td>6.50</td>\n",
       "      <td>8.30</td>\n",
       "      <td>7.30</td>\n",
       "      <td>6.40</td>\n",
       "      <td>6.90</td>\n",
       "      <td>8.10</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BE</th>\n",
       "      <td>-4.70</td>\n",
       "      <td>-4.20</td>\n",
       "      <td>-7.40</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>-6.40</td>\n",
       "      <td>-5.90</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-11.20</td>\n",
       "      <td>-3.10</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.90</td>\n",
       "      <td>-4.10</td>\n",
       "      <td>-7.30</td>\n",
       "      <td>-8.00</td>\n",
       "      <td>-7.90</td>\n",
       "      <td>-4.50</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-7.20</td>\n",
       "      <td>-3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apgar1</th>\n",
       "      <td>9.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 552 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1220  1234  1208   1038  1004  1010   1022   1036  2041  1206  ...  \\\n",
       "parameter                                                                 ...   \n",
       "pH         7.30  7.29  7.23   7.33  7.30  7.35   7.28   7.08  7.37  7.24  ...   \n",
       "BDecf      3.52  2.50  5.84   2.72  5.19  5.20   1.53   8.11  3.69  2.06  ...   \n",
       "pCO2       6.00  6.50  6.60   5.70  5.50  4.70   7.00   9.30  4.80  7.70  ...   \n",
       "BE        -4.70 -4.20 -7.40  -4.00 -6.40 -5.90  -3.00 -11.20 -3.10 -4.00  ...   \n",
       "Apgar1     9.00  8.00  9.00  10.00  8.00  8.00  10.00   8.00  9.00  9.00  ...   \n",
       "\n",
       "           1079  1290   1284  1077  1063  1088  1253  1247  2028   2014  \n",
       "parameter                                                                \n",
       "pH         7.17  7.36   7.16  7.22  7.16  7.25  7.32  7.32  7.18   7.32  \n",
       "BDecf      7.91  3.88   5.07  6.69  5.56  2.58  0.89 -0.86  4.82   2.28  \n",
       "pCO2       7.10  4.90   8.50  6.50  8.30  7.30  6.40  6.90  8.10   6.00  \n",
       "BE        -9.90 -4.10  -7.30 -8.00 -7.90 -4.50 -1.70 -1.00 -7.20  -3.20  \n",
       "Apgar1     8.00  9.00  10.00  6.00  9.00  8.00  8.00  9.00  8.00  10.00  \n",
       "\n",
       "[5 rows x 552 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load meta data and transform\n",
    "metadata = pd.read_csv(os.path.join(paths.data, paths.meta),\n",
    "                       index_col='parameter')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate expert feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m csv_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# ct += 1\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# print(f'{key}/{value}')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     fhr_dict[key] \u001b[38;5;241m=\u001b[39m clean_fhr(value\u001b[38;5;241m.\u001b[39mFHR)\n\u001b[0;32m---> 10\u001b[0m     feature_dict[key] \u001b[38;5;241m=\u001b[39m calculate_features(fhr_dict[key])\n",
      "File \u001b[0;32m~/Desktop/科研/fMRI/胎心监护-龙岗妇幼/公开数据集/utils/calculate_expert_feature.py:337\u001b[0m, in \u001b[0;36mcalculate_features\u001b[0;34m(fhr_data, sampling_rate)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_features\u001b[39m(fhr_data, sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m--> 337\u001b[0m     feature_1 \u001b[38;5;241m=\u001b[39m calculate_hrr_features(fhr_data)\n\u001b[1;32m    338\u001b[0m     feature_2 \u001b[38;5;241m=\u001b[39m calculate_frequency_domain_features(fhr_data\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m    339\u001b[0m     feature_3 \u001b[38;5;241m=\u001b[39m calculate_nonlinear_features(fhr_data)\n",
      "File \u001b[0;32m~/Desktop/科研/fMRI/胎心监护-龙岗妇幼/公开数据集/utils/calculate_expert_feature.py:143\u001b[0m, in \u001b[0;36mcalculate_hrr_features\u001b[0;34m(fhr_signal)\u001b[0m\n\u001b[1;32m    140\u001b[0m std_baseline \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(fhr_baseline)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# 加速和减速事件\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m nACC, nDEC \u001b[38;5;241m=\u001b[39m calculate_nAcc_nDec(fhr_signal, mean_baseline)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# 计算RR间期序列的统计指标\u001b[39;00m\n\u001b[1;32m    146\u001b[0m rr_intervals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60000\u001b[39m \u001b[38;5;241m/\u001b[39m fhr_signal\n",
      "File \u001b[0;32m~/Desktop/科研/fMRI/胎心监护-龙岗妇幼/公开数据集/utils/calculate_expert_feature.py:116\u001b[0m, in \u001b[0;36mcalculate_nAcc_nDec\u001b[0;34m(fhr_signal, baseline, sampling_rate)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmin\u001b[39m(start \u001b[38;5;241m+\u001b[39m duration_threshold_10m, \u001b[38;5;28mlen\u001b[39m(fhr_signal))):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dec_baseline[end]:\n\u001b[0;32m--> 116\u001b[0m         duration2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m         start \u001b[38;5;241m=\u001b[39m end\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.calculate_expert_feature import calculate_features\n",
    "from utils.clean_fhr import clean_fhr\n",
    "fhr_dict = {}\n",
    "feature_dict = {}\n",
    "# ct = 0\n",
    "for key, value in csv_files.items():\n",
    "    # ct += 1\n",
    "    # print(f'{key}/{value}')\n",
    "    fhr_dict[key] = clean_fhr(value.FHR)\n",
    "    feature_dict[key] = calculate_features(fhr_dict[key])\n",
    "    # if True:\n",
    "        # print(f'{ct}/{len(csv_files.items())}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set x as the signals from the dictionary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mfeature_dict\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(X_feature[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Y is a boolean, true/false for each depending on outcome\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# We set order of metadata according to dictionary keys so they match up\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Set x as the signals from the dictionary\n",
    "X_feature = list(feature_dict.values())\n",
    "print(type(X_feature[0]))\n",
    "\n",
    "# Y is a boolean, true/false for each depending on outcome\n",
    "# We set order of metadata according to dictionary keys so they match up\n",
    "y = (metadata[X_feature.keys()].loc[outcome] < outcome_threshold).values\n",
    "y = y.astype(np.int8)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mX_feature\u001b[49m, y, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mbincount(y_train))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mbincount(y_test))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_feature' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_feature, y, test_size = 0.25, random_state=42)\n",
    "\n",
    "print(np.bincount(y_train))\n",
    "print(np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    \"\"\"Scale data 0-1 based on min and max in training set\"\"\"\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = MinMaxScaler()\n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_sc = sc.transform(X_train)\n",
    "    test_sc = sc.transform(X_test)\n",
    "    \n",
    "    return train_sc, test_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Scale X data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train_sc, X_test_sc \u001b[38;5;241m=\u001b[39m scale_data(\u001b[43mX_train\u001b[49m, X_test)\n\u001b[1;32m      4\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe number of train data\u001b[39m\u001b[38;5;124m'\u001b[39m, X_train_sc\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Scale X data\n",
    "X_train_sc, X_test_sc = scale_data(X_train, X_test)\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "print('the number of train data', X_train_sc.shape)\n",
    "print('the number of test data', X_test_sc.shape)\n",
    "print(type(X_train_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# from xgboost import XGBClassifier\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# KNN\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model_knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m model_knn\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_sc\u001b[49m, y_train)\n\u001b[1;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_knn\u001b[38;5;241m.\u001b[39mpredict(X_test_sc)\n\u001b[1;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mround\u001b[39m(value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m y_pred]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_sc' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import (GridSearchCV, cross_val_score,\n",
    "                                     train_test_split)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from utils import evaluate\n",
    "# from xgboost import XGBClassifier\n",
    "# KNN\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn.fit(X_train_sc, y_train)\n",
    "\n",
    "y_pred = model_knn.predict(X_test_sc)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "evaluate.evaluate_model(model_knn, X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FHR classify base on SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_svc \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model_svc\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_sc\u001b[49m, y_train)\n\u001b[1;32m      4\u001b[0m evaluate\u001b[38;5;241m.\u001b[39mevaluate_model(model_svc, X_test_sc, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_sc' is not defined"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(kernel='linear')\n",
    "model_svc.fit(X_train_sc, y_train)\n",
    "\n",
    "evaluate.evaluate_model(model_svc, X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_sc\u001b[49m, y_train)\n\u001b[1;32m      4\u001b[0m evaluate\u001b[38;5;241m.\u001b[39mevaluate_model(model, X_test_sc, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_sc' is not defined"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='rbf')\n",
    "model.fit(X_train_sc, y_train)\n",
    "\n",
    "evaluate.evaluate_model(model, X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_rfc \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m----> 2\u001b[0m model_rfc\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_sc\u001b[49m, y_train)\n\u001b[1;32m      4\u001b[0m evaluate\u001b[38;5;241m.\u001b[39mevaluate_model(model_rfc, X_test_sc, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_sc' is not defined"
     ]
    }
   ],
   "source": [
    "model_rfc = RandomForestClassifier()\n",
    "model_rfc.fit(X_train_sc, y_train)\n",
    "\n",
    "evaluate.evaluate_model(model_rfc, X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m      2\u001b[0m model_xg \u001b[38;5;241m=\u001b[39m XGBClassifier(eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m, objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m'\u001b[39m, use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m model_xg\u001b[38;5;241m.\u001b[39mfit(X_train_sc, y_train)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model_xg = XGBClassifier(eval_metric='mlogloss', objective='binary:logistic', use_label_encoder=False)\n",
    "model_xg.fit(X_train_sc, y_train)\n",
    "\n",
    "evaluate.evaluate_model(model_xg, X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_xg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate\u001b[38;5;241m.\u001b[39mplot_auc_roc([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mmodel_xg\u001b[49m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m, model_rfc), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupportVectorMachines\u001b[39m\u001b[38;5;124m'\u001b[39m, model_svc), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKNN\u001b[39m\u001b[38;5;124m'\u001b[39m, model_knn)], X_test_sc, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_xg' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate.plot_auc_roc([('XGBoost', model_xg), ('RandomForest', model_rfc), ('SupportVectorMachines', model_svc), ('KNN', model_knn)], X_test_sc, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
